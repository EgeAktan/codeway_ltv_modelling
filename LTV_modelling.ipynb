{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# LTV Prediction Case Study â€“ Codeway\n",
    "# Author: Ege Aktan\n",
    "# ================================================================\n",
    "\n",
    "# ================================================================\n",
    "# Imports\n",
    "# ================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# User Input Parameters\n",
    "# ================================================================\n",
    "FILE_PATH = Path()\n",
    "OUTPUT_PATH = Path()  \n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Data Loading & Preprocessing\n",
    "# ================================================================\n",
    "FILE_PATH = \"/Users/egeaktan/Desktop/WORK/Codeway/ltvprediction_case.parquet\"\n",
    "df = pd.read_parquet(FILE_PATH, engine=\"fastparquet\")\n",
    "\n",
    "df[\"revenue\"] = df[\"revenue\"].fillna(0)\n",
    "df[\"event_date\"] = pd.to_datetime(df[\"event_date\"])\n",
    "df[\"first_event_date\"] = pd.to_datetime(df[\"first_event_date\"])\n",
    "df[\"event_time\"] = pd.to_datetime(df[\"event_time\"], unit=\"ms\")   \n",
    "df[\"event_hour\"] = df[\"event_time\"].dt.hour\n",
    "df[\"tenure_days_row\"] = (df[\"event_date\"] - df[\"first_event_date\"]).dt.days\n",
    "df[\"days_since_install\"] = (df[\"event_date\"] - df[\"first_event_date\"]).dt.days\n",
    "\n",
    "df[\"country\"] = df[\"country\"].fillna(\"UNKNOWN\")\n",
    "df.loc[df[\"operating_system\"] == \"ios\", \"operating_system\"] = \"iOS\"\n",
    "bad_users = (\n",
    "    df.groupby(\"user_id\")[\"first_event_date\"]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .query(\"first_event_date > 1\")[\"user_id\"]\n",
    ")\n",
    "df = df[~df[\"user_id\"].isin(bad_users)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Feature Generation\n",
    "# ================================================================\n",
    "journey_table = (\n",
    "    df.pivot_table(index=\"user_id\", columns=\"event_name\", values=\"event_date\",\n",
    "                     aggfunc=\"count\", fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "journey_table.loc[\n",
    "    (journey_table.get(\"auto_renew_off\", 0) > 0) |\n",
    "    (journey_table.get(\"renewal\", 0) > 0) |\n",
    "    (journey_table.get(\"refund\", 0) > 0),\n",
    "    \"subscribe\"\n",
    "] = journey_table.get(\"subscribe\", 0).where(journey_table.get(\"subscribe\", 0) > 0, 1)\n",
    "\n",
    "binary_event_cols =[\"auto_renew_off\", \"renewal\", \"refund\", \"subscribe\", \"free_trial\"]\n",
    "journey_binary = journey_table.copy()\n",
    "journey_binary[binary_event_cols] = (journey_binary[binary_event_cols] > 0).astype(int)\n",
    "\n",
    "tenure_df = (\n",
    "    df.groupby(\"user_id\").agg(\n",
    "        app_launch_date=(\"event_date\", \"min\"),\n",
    "        last_event_date=(\"event_date\", \"max\"),\n",
    "        install_date=(\"first_event_date\", \"min\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "tenure_df[\"tenure_days\"] = (tenure_df[\"last_event_date\"] - tenure_df[\"install_date\"]).dt.days\n",
    "tenure_df[\"days_until_app_launch\"] = (tenure_df[\"app_launch_date\"] - tenure_df[\"install_date\"]).dt.days\n",
    "\n",
    "df_sorted = df.sort_values([\"user_id\", \"event_time\"])[[\"user_id\",\"event_time\"]]\n",
    "df_sorted[\"time_diff_min\"] = (\n",
    "    df_sorted.groupby(\"user_id\")[\"event_time\"]\n",
    "             .diff() / pd.Timedelta(minutes=1)\n",
    ")\n",
    "\n",
    "avg_inter_event = (\n",
    "    df_sorted.groupby(\"user_id\")[\"time_diff_min\"]\n",
    "    .mean()\n",
    "    .reset_index(name=\"avg_inter_event_time\")\n",
    ")\n",
    "\n",
    "country_user_stats = (\n",
    "    df.groupby([\"user_id\"]).agg(\n",
    "        country = (\"country\", \"first\"),\n",
    "        country_user_rev=(\"revenue\", \"sum\")\n",
    "        )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "country_stats = (\n",
    "    country_user_stats.groupby([\"country\"]).agg(\n",
    "        country_avg_rev=(\"country_user_rev\",\"mean\"),\n",
    "        country_std_rev=(\"country_user_rev\",\"std\")\n",
    "        )\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "country_stats[\"country_var_rev\"] = (\n",
    "    country_stats[\"country_avg_rev\"] / country_stats[\"country_std_rev\"]\n",
    ")\n",
    "\n",
    "country_user_stats = country_user_stats[[\"user_id\", \"country\"]].merge(country_stats, on= \"country\", how=\"left\")\n",
    "\n",
    "user_os = df[[\"user_id\", \"operating_system\"]].groupby(\"user_id\").first()[\"operating_system\"]\n",
    "\n",
    "ios_flag = (user_os.isin([\"iOS\", \"iPadOS\"])).astype(int).reset_index(name=\"ios_flag\")\n",
    "\n",
    "df_daily_max_events = (\n",
    "    df.groupby([\"user_id\", \"event_date\"]).size().groupby(\"user_id\").max()\n",
    "      .reset_index(name=\"daily_max_events\")\n",
    ")\n",
    "\n",
    "subs_first = (\n",
    "    df[df[\"event_name\"].isin([\"subscribe\", \"auto_renew_off\", \"renewal\", \"refund\"])]\n",
    "      .groupby(\"user_id\")[\"event_time\"].min()\n",
    "      .reset_index(name=\"first_subscribe_time\")\n",
    ")\n",
    "\n",
    "installs = (\n",
    "    df.groupby(\"user_id\")[\"first_event_date\"].min()\n",
    "      .reset_index(name=\"install_date\")\n",
    ")\n",
    "installs[\"install_date\"] = pd.to_datetime(installs[\"install_date\"])\n",
    "time_to_first_subscribe = subs_first.merge(installs, on=\"user_id\", how=\"left\")\n",
    "time_to_first_subscribe[\"time_to_first_subscribe\"] = (\n",
    "    time_to_first_subscribe[\"first_subscribe_time\"] - time_to_first_subscribe[\"install_date\"]\n",
    ").dt.days\n",
    "time_to_first_subscribe = time_to_first_subscribe[[\"user_id\", \"time_to_first_subscribe\"]]\n",
    "\n",
    "revenue_15days = (df.groupby(\"user_id\")[\"revenue\"].sum().reset_index(name=\"revenue_15days\"))\n",
    "\n",
    "\n",
    "trial_first = (\n",
    "    df[df[\"event_name\"] == \"free_trial\"]\n",
    "      .groupby(\"user_id\")[\"event_time\"].min()\n",
    "      .reset_index(name=\"first_trial_time\")\n",
    ")\n",
    "\n",
    "time_to_first_trial = trial_first.merge(installs, on=\"user_id\", how=\"left\")\n",
    "time_to_first_trial[\"time_to_first_trial\"] = (\n",
    "    time_to_first_trial[\"first_trial_time\"] - time_to_first_trial[\"install_date\"]\n",
    ").dt.days\n",
    "time_to_first_trial = time_to_first_trial[[\"user_id\", \"time_to_first_trial\"]]\n",
    "\n",
    "\n",
    "bins   = [-1, 7, 15, 23]\n",
    "labels = [\"Early\", \"Daytime\", \"Evening\"]\n",
    "\n",
    "df[\"event_hour_bin\"] = pd.cut(df[\"event_hour\"], bins=bins, labels=labels)\n",
    "df[\"event_hour_bin\"] = df[\"event_hour_bin\"].astype(pd.CategoricalDtype(categories=labels, ordered=True))\n",
    "\n",
    "event_hour_df = (\n",
    "    df.groupby(\"user_id\")[\"event_hour_bin\"]\n",
    "      .value_counts(normalize=True)\n",
    "      .unstack(fill_value=0)\n",
    "      .reindex(columns=labels, fill_value=0) \n",
    "      .add_prefix(\"ratio_\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "agg_user = (\n",
    "    df.groupby(\"user_id\").agg(\n",
    "        active_days=(\"event_date\", \"nunique\"),\n",
    "        events_total=(\"event_name\", \"count\"),\n",
    "        first_year_revenue=(\"first_year_revenue\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "days_since = (df[\"event_time\"] - df[\"first_event_date\"]).dt.days\n",
    "windows = [1, 3, 7]\n",
    "out = []\n",
    "\n",
    "for w in windows:\n",
    "    s = (\n",
    "        df.loc[days_since.between(0, w, inclusive=\"both\")]\n",
    "          .groupby(\"user_id\")[\"revenue\"].sum()\n",
    "          .rename(f\"revenue_d{w}\")\n",
    "    )\n",
    "    out.append(s)\n",
    "\n",
    "rev_windows = pd.concat(out, axis=1).fillna(0).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Joining Features & Further Preprocessing\n",
    "# ================================================================\n",
    "\n",
    "master_df = (\n",
    "    journey_binary\n",
    "    .merge(tenure_df[[\"user_id\", \"tenure_days\", \"days_until_app_launch\"]], on=\"user_id\", how=\"left\")\n",
    "    .merge(agg_user, on=\"user_id\", how=\"left\")\n",
    "    .merge(df_daily_max_events, on=\"user_id\", how=\"left\")\n",
    "    .merge(avg_inter_event, on=\"user_id\", how=\"left\")\n",
    "    .merge(time_to_first_subscribe, on=\"user_id\", how=\"left\")\n",
    "    .merge(revenue_15days, on=\"user_id\", how=\"left\")\n",
    "    .merge(time_to_first_trial, on=\"user_id\", how=\"left\")\n",
    "    .merge(event_hour_df, on=\"user_id\", how=\"left\")\n",
    "    .merge(rev_windows, on=\"user_id\", how=\"left\")\n",
    "    .merge(ios_flag, on=\"user_id\", how=\"left\")\n",
    "    .merge(country_user_stats, on=\"user_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "bin_cols = [\"auto_renew_off\", \"renewal\", \"refund\", \"subscribe\", \"free_trial\"]\n",
    "bin_cols = [c for c in bin_cols if c in master_df.columns]\n",
    "master_df[bin_cols] = master_df[bin_cols].fillna(0).astype(int)\n",
    "\n",
    "ratio_cols = [c for c in master_df.columns if c.startswith(\"ratio_\")]\n",
    "master_df[ratio_cols] = master_df[ratio_cols].fillna(0.0)\n",
    "\n",
    "master_df[[\"revenue_d1\", \"revenue_d3\", \"revenue_d7\"]] = master_df[[\"revenue_d1\", \"revenue_d3\", \"revenue_d7\"]].fillna(0)\n",
    "master_df[\"country_var_rev\"] = master_df.country_var_rev.fillna(master_df.country_var_rev.max())\n",
    "\n",
    "master_df = master_df[\n",
    "    (master_df[[\"first_year_revenue\"]] >= 0).all(axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 554\n",
      "[LightGBM] [Info] Number of data points in the train set: 1407878, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egeaktan/Desktop/BSBI/DISSERTATION/Project/CLV/lib/python3.11/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- Prep ---------------------------------------------------------------\n",
    "dfm = master_df.copy()\n",
    "dfm = dfm[dfm[\"events_total\"] > 0].copy()\n",
    "dfm[\"payer_flag\"] = (dfm[\"first_year_revenue\"] > 0).astype(int)\n",
    "\n",
    "# normalize revenue snapshots only for payers\n",
    "mask = dfm[\"payer_flag\"] > 0\n",
    "for c in [\"revenue_d1\", \"revenue_d3\", \"revenue_d7\"]:\n",
    "    if c in dfm.columns and \"revenue_15days\" in dfm.columns:\n",
    "        dfm.loc[mask, c] = dfm.loc[mask, c] / dfm.loc[mask, \"revenue_15days\"]\n",
    "\n",
    "clf_features = [\n",
    "    \"tenure_days\",\"free_trial\",\"paywall\",\n",
    "    \"active_days\",\"events_total\",\"daily_max_events\",\n",
    "    \"avg_inter_event_time\",\"time_to_first_trial\",\"ios_flag\",\n",
    "    \"ratio_Evening\",\"ratio_Early\",\"ratio_Daytime\",\"country_var_rev\"\n",
    "]\n",
    "clf_features = [c for c in clf_features if c in dfm.columns]\n",
    "\n",
    "reg_features = [\n",
    "    \"revenue_15days\",\"tenure_days\",\"revenue_d7\",\n",
    "    \"avg_inter_event_time\",\"time_to_first_subscribe\",\n",
    "    \"revenue_d1\",\"ratio_Evening\",\"ratio_Early\",\"ratio_Daytime\",\n",
    "    \"daily_max_events\",\"events_total\",\"active_days\",\"country_avg_rev\",\"ios_flag\"\n",
    "]\n",
    "reg_features = [c for c in reg_features if c in dfm.columns]\n",
    "\n",
    "def sanitize(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure clean floats for ML.\"\"\"\n",
    "    return X.replace([np.inf, -np.inf], np.nan).astype(np.float32)\n",
    "\n",
    "dfm.loc[:, clf_features] = sanitize(dfm[clf_features])\n",
    "dfm.loc[:, reg_features] = sanitize(dfm[reg_features])\n",
    "\n",
    "idx_tr, idx_hold = train_test_split(\n",
    "    dfm.index, test_size=0.20, stratify=dfm[\"payer_flag\"], random_state=42\n",
    ")\n",
    "df_tr, df_hold = dfm.loc[idx_tr].copy(), dfm.loc[idx_hold].copy()\n",
    "\n",
    "# =========================\n",
    "# Stage 1: Multiclass classifier + calibration\n",
    "# =========================\n",
    "pos_rev = df_tr.loc[df_tr[\"first_year_revenue\"] > 0, \"first_year_revenue\"]\n",
    "whale_cut = np.percentile(pos_rev, 80) if len(pos_rev) > 0 else 0.0  \n",
    "\n",
    "def label_multiclass(r):\n",
    "    if r <= 0: return 0\n",
    "    return 2 if r >= whale_cut else 1\n",
    "\n",
    "df_tr[\"y_cls\"] = df_tr[\"first_year_revenue\"].apply(label_multiclass).astype(int)\n",
    "\n",
    "Xc_tr, Xc_cal, yc_tr, yc_cal = train_test_split(\n",
    "    df_tr[clf_features], df_tr[\"y_cls\"],\n",
    "    test_size=0.20, stratify=df_tr[\"y_cls\"], random_state=42\n",
    ")\n",
    "\n",
    "cls_counts = Counter(yc_tr)\n",
    "total = len(yc_tr)\n",
    "class_weight = {k: total / (3 * cls_counts.get(k, 1)) for k in [0, 1, 2]}\n",
    "\n",
    "clf_base = lgb.LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    num_class=3,\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.08,\n",
    "    num_leaves=31,\n",
    "    max_bin=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    class_weight=class_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "clf_base.fit(\n",
    "    Xc_tr, yc_tr,\n",
    "    eval_set=[(Xc_cal, yc_cal)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    ")\n",
    "\n",
    "cal_counts = Counter(yc_cal)\n",
    "\n",
    "cal_clf = CalibratedClassifierCV(\n",
    "    estimator=clf_base,\n",
    "    method=\"sigmoid\" ,\n",
    "    cv=\"prefit\"\n",
    ")\n",
    "cal_clf.fit(Xc_cal, yc_cal)\n",
    "\n",
    "proba_tr = cal_clf.predict_proba(df_tr[clf_features])\n",
    "df_tr[\"p_nonpayer\"] = proba_tr[:, 0]\n",
    "df_tr[\"p_regular\"]  = proba_tr[:, 1]\n",
    "df_tr[\"p_whale\"]    = proba_tr[:, 2]\n",
    "df_tr[\"p_payer\"]    = df_tr[\"p_regular\"] + df_tr[\"p_whale\"]\n",
    "\n",
    "proba_hold = cal_clf.predict_proba(df_hold[clf_features])\n",
    "df_hold[\"p_nonpayer\"] = proba_hold[:, 0]\n",
    "df_hold[\"p_regular\"]  = proba_hold[:, 1]\n",
    "df_hold[\"p_whale\"]    = proba_hold[:, 2]\n",
    "df_hold[\"p_payer\"]    = df_hold[\"p_regular\"] + df_hold[\"p_whale\"]\n",
    "\n",
    "\n",
    "reg_plus = reg_features + [\"p_whale\"]\n",
    "pay_tr = df_tr[df_tr[\"first_year_revenue\"] > 0].copy()\n",
    "\n",
    "\n",
    "y_raw = pay_tr[\"first_year_revenue\"].values\n",
    "cap99 = np.percentile(y_raw, 99)\n",
    "y = np.clip(y_raw, 0, cap99)\n",
    "\n",
    "Xr_tr, Xr_val, yr_tr, yr_val = train_test_split(\n",
    "    pay_tr[reg_plus], y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "dtrain = xgb.DMatrix(Xr_tr, label=yr_tr)\n",
    "dvalid = xgb.DMatrix(Xr_val, label=yr_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",  \n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"eta\": 0.05,          \n",
    "    \"max_depth\": 7,      \n",
    "    \"min_child_weight\": 3,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"lambda\": 1.0,        \n",
    "    \"alpha\": 0.0,        \n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(dvalid, \"valid\")],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False,\n",
    ")\n",
    "\n",
    "X_hold_reg = xgb.DMatrix(df_hold[reg_plus])\n",
    "df_hold[\"amount_hat\"] = np.clip(bst.predict(X_hold_reg), 0, None)\n",
    "\n",
    "df_hold[\"expected_ltv\"] = df_hold[\"p_payer\"] * df_hold[\"amount_hat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = dfm.copy()\n",
    "\n",
    "proba = cal_clf.predict_proba(df_all[clf_features])\n",
    "df_all[\"p_nonpayer\"] = proba[:, 0]\n",
    "df_all[\"p_regular\"]  = proba[:, 1]\n",
    "df_all[\"p_whale\"]    = proba[:, 2]\n",
    "df_all[\"p_payer\"]    = df_all[\"p_regular\"] + df_all[\"p_whale\"]\n",
    "\n",
    "X_hold_reg = xgb.DMatrix(df_all[reg_plus])\n",
    "df_all[\"amount_hat\"] = np.clip(bst.predict(X_hold_reg), 0, None)\n",
    "df_all[\"expected_ltv\"] = df_all[\"p_payer\"] * df_all[\"amount_hat\"]\n",
    "\n",
    "df_all = df_all.merge(tenure_df[[\"user_id\", \"install_date\"]], on=\"user_id\", how=\"left\")\n",
    "\n",
    "df_result = df_all[[\"user_id\", \"country\", \"install_date\", \"expected_ltv\"]].rename(\n",
    "    columns={\"install_date\": \"first_event_date\",\n",
    "             \"expected_ltv\": \"predicted_ltv\"}\n",
    ")\n",
    "\n",
    "output_file = OUTPUT_PATH / \"predicted_ltv.parquet\"\n",
    "df_result.to_parquet(output_file, index=False, engine=\"fastparquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
